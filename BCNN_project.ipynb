{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61780bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import data, metrics\n",
    "import Frequentist_main as FCNN\n",
    "import Bayesian_main as BCNN\n",
    "from Bayesian.BayesianCNN import BBBAlexNet\n",
    "from Frequentist.FrequentistCNN import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e0e982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca9cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "\n",
    "n_epochs = 5\n",
    "lr_start = 0.001\n",
    "num_workers = 4\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc632e0-833b-470d-9139-824f2bfcb165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "c10_trainset, c10_testset, c10_inputs, c10_outputs = data.getDataset('CIFAR10')\n",
    "c10_train_loader, c10_valid_loader, c10_test_loader = data.getDataloader(\n",
    "    c10_trainset, c10_testset, valid_size, batch_size, num_workers)\n",
    "\n",
    "c100_trainset, c100_testset, c100_inputs, c100_outputs = data.getDataset('CIFAR100')\n",
    "c100_train_loader, c100_valid_loader, c100_test_loader = data.getDataloader(\n",
    "    c100_trainset, c100_testset, valid_size, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6789e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BayesianCNN with softplus on CIFAR10\n",
    "bc10_sp_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='softplus').to(device)\n",
    "bc10_sp_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "bc10_sp_optimizer = Adam(bc10_sp_net.parameters(), lr=lr_start)\n",
    "bc10_sp_lr_sched = lr_scheduler.ReduceLROnPlateau(bc10_sp_optimizer, patience=6, verbose=True)\n",
    "valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc10_train_loss, bc10_train_acc, bc10_train_kl = BCNN.train_model(bc10_sp_net, bc10_sp_optimizer, bc10_sp_criterion, c10_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc10_valid_loss, bc10_valid_acc = BCNN.validate_model(bc10_sp_net, bc10_sp_criterion, c10_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    lr_sched.step(bc10_valid_loss)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc10_train_loss, bc10_train_acc, bc10_valid_loss, bc10_valid_acc, bc10_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891eb21e-d97f-40e2-b041-524dc174b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bc100_sp_net = BBBAlexNet(c100_outputs, c100_inputs, priors, activation_type='softplus').to(device)\n",
    "bc100_sp_criterion = metrics.ELBO(len(c100_trainset)).to(device)\n",
    "bc100_sp_optimizer = Adam(bc100_sp_net.parameters(), lr=lr_start)\n",
    "bc100_sp_lr_sched = lr_scheduler.ReduceLROnPlateau(bc100_sp_optimizer, patience=6, verbose=True)\n",
    "valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc100_train_loss, bc100_train_acc, bc100_train_kl = BCNN.train_model(bc100_sp_net, bc100_sp_optimizer, bc100_sp_criterion, c100_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc100_valid_loss, bc100_valid_acc = BCNN.validate_model(bc100_sp_net, bc100_sp_criterion, c100_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    lr_sched.step(bc100_valid_loss)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc100_train_loss, bc100_train_acc, bc100_valid_loss, bc100_valid_acc, bc100_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750fe94-db5a-4d5b-968e-fbc28d3bb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "bc10_rl_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='relu').to(device)\n",
    "bc10_rl_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "bc10_rl_optimizer = Adam(bc10_rl_net.parameters(), lr=lr_start)\n",
    "bc10_rl_lr_sched = lr_scheduler.ReduceLROnPlateau(bc10_rl_optimizer, patience=6, verbose=True)\n",
    "valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc10_train_loss, bc10_train_acc, bc10_train_kl = BCNN.train_model(bc10_rl_net, bc10_rl_optimizer, bc10_rl_criterion, c10_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc10_valid_loss, bc10_valid_acc = BCNN.validate_model(bc10_rl_net, bc10_rl_criterion, c10_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    lr_sched.step(bc10_valid_loss)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc10_train_loss, bc10_train_acc, bc10_valid_loss, bc10_valid_acc, bc10_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8afc2-14dd-4aeb-8242-36f950285847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "bc100_rl_net = BBBAlexNet(c100_outputs, c100_inputs, priors, activation_type='relu').to(device)\n",
    "bc100_rl_criterion = metrics.ELBO(len(c100_trainset)).to(device)\n",
    "bc100_rl_optimizer = Adam(bc100_rl_net.parameters(), lr=lr_start)\n",
    "bc100_rl_lr_sched = lr_scheduler.ReduceLROnPlateau(bc100_rl_optimizer, patience=6, verbose=True)\n",
    "valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc100_train_loss, bc100_train_acc, bc100_train_kl = BCNN.train_model(bc100_rl_net, bc100_rl_optimizer, bc100_rl_criterion, c100_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc100_valid_loss, bc100_valid_acc = BCNN.validate_model(bc100_rl_net, bc100_rl_criterion, c100_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    lr_sched.step(bc100_valid_loss)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc100_train_loss, bc100_train_acc, bc100_valid_loss, bc100_valid_acc, bc100_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79873-300a-410f-851d-1cb2f1e829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fc10_net = AlexNet(c10_outputs, c10_inputs).to(device)\n",
    "fc10_criterion = nn.CrossEntropyLoss()\n",
    "fc10_optimizer = Adam(fc10_net.parameters(), lr=lr_start)\n",
    "lr_sched = lr_scheduler.ReduceLROnPlateau(fc10_optimizer, patience=6, verbose=True)\n",
    "valid_loss_min = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc10_train_loss, fc10_train_acc = FCNN.train_model(fc10_net, fc10_optimizer, fc10_criterion, c10_train_loader)\n",
    "    fc10_valid_loss, fc10_valid_acc = FCNN.validate_model(fc10_net, fc10_criterion, c10_valid_loader)\n",
    "    lr_sched.step(fc10_valid_loss)\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc10_train_loss, fc10_train_acc, fc10_valid_loss, fc10_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f645b1-2c30-4fc8-aa1d-f32574fa7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fc100_net = AlexNet(c100_outputs, c100_inputs).to(device)\n",
    "fc100_criterion = nn.CrossEntropyLoss()\n",
    "fc100_optimizer = Adam(fc100_net.parameters(), lr=lr_start)\n",
    "fc100_lr_sched = lr_scheduler.ReduceLROnPlateau(fc100_optimizer, patience=6, verbose=True)\n",
    "valid_loss_min = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc100_train_loss, fc100_train_acc = FCNN.train_model(fc100_net, fc100_optimizer, fc100_criterion, c100_train_loader)\n",
    "    fc100_valid_loss, fc100_valid_acc = FCNN.validate_model(fc100_net, fc100_criterion, c100_valid_loader)\n",
    "    lr_sched.step(fc100_valid_loss)\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc100_train_loss, fc100_train_acc, fc100_valid_loss, fc100_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33529bcf-0a43-47f1-8035-5c016b3fe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 201)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, bc10_sp_train_acc, label='bc10_sp')\n",
    "plt.plot(epochs, bc100_sp_train_acc, label='bc100_sp')\n",
    "plt.plot(epochs, bc10_rl_train_acc, label='bc10_rl')\n",
    "plt.plot(epochs, bc100_rl_train_acc, label='bc100_rl')\n",
    "plt.plot(epochs, fc10_train_acc, label='fc10')\n",
    "plt.plot(epochs, fc100_train_acc, label='fc100')\n",
    "\n",
    "plt.title('Comparison of training accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1caa85-a79d-4039-919b-5d2bde171785",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 201)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, bc10_sp_valid_acc, label='bc10_sp')\n",
    "plt.plot(epochs, bc100_sp_valid_acc, label='bc100_sp')\n",
    "plt.plot(epochs, bc10_rl_train_acc, label='bc10_rl')\n",
    "plt.plot(epochs, bc100_rl_train_acc, label='bc100_rl')\n",
    "plt.plot(epochs, fc10_valid_acc, label='fc10')\n",
    "plt.plot(epochs, fc100_valid_acc, label='fc100')\n",
    "\n",
    "plt.title('Comparison of validation accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
