{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61780bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import itertools\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from utils import data, metrics\n",
    "import Frequentist_main as FCNN\n",
    "import Bayesian_main as BCNN\n",
    "from Bayesian.BayesianCNN import BBBAlexNet\n",
    "from Frequentist.FrequentistCNN import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e0e982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca9cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "\n",
    "n_epochs = 100\n",
    "lr_start = [0.005, 0.001, 0.0005]\n",
    "batch_size = [128, 256, 512]\n",
    "hp_tuning = list(itertools.product(lr_start, batch_size))\n",
    "valid_size = 0.2\n",
    "beta_type = \"Blundell\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc632e0-833b-470d-9139-824f2bfcb165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Dataset and Dataloader\n",
    "c10_trainset, c10_testset, c10_inputs, c10_outputs = data.getDataset('CIFAR10')\n",
    "c100_trainset, c100_testset, c100_inputs, c100_outputs = data.getDataset('CIFAR100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6789e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BayesianCNN with softplus on CIFAR10 hyperparametrization tuning\n",
    "for lr, batch in hp_tuning:\n",
    "    filename = f\"results_lr{lr}_batch{batch}.csv\"\n",
    "\n",
    "    c10_train_loader, c10_valid_loader, _ = data.getDataloader(\n",
    "    c10_trainset, c10_testset, valid_size, batch)\n",
    "    \n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Train KL Div'])\n",
    "        \n",
    "        bc10_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='softplus').to(device)\n",
    "        bc10_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "        bc10_optimizer = Adam(bc10_net.parameters(), lr=lr)\n",
    "        bc10_lr_sched = lr_scheduler.ReduceLROnPlateau(bc10_optimizer, patience=6, verbose=True)\n",
    "        bc10_valid_loss_max = np.Inf\n",
    "\n",
    "        ckpt_name = f\"Bayesian/Models/bc10_lr{lr}_batch{batch}.pth\"\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            bc10_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            bc10_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            bc10_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            bc10_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0 \n",
    "\n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):  # loop over the dataset multiple times\n",
    "        \n",
    "            bc10_train_loss, bc10_train_acc, bc10_train_kl = BCNN.train_model(bc10_net, bc10_optimizer, bc10_criterion, c10_train_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc10_valid_loss, bc10_valid_acc = BCNN.validate_model(bc10_net, bc10_criterion, c10_valid_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc10_lr_sched.step(bc10_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if bc10_valid_loss <= bc10_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    bc10_valid_loss_max, bc10_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': bc10_net.state_dict(),\n",
    "                    'optimizer_state_dict': bc10_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': bc10_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': bc10_valid_loss,\n",
    "                    'epoch': epoch\n",
    "                }, ckpt_name)\n",
    "                bc10_valid_loss_max = bc10_valid_loss\n",
    "\n",
    "            writer.writerow([epoch, bc10_train_loss, bc10_train_acc, bc10_valid_loss, bc10_valid_acc, bc10_train_kl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891eb21e-d97f-40e2-b041-524dc174b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BayesianCNN with softplus on CIFAR100\n",
    "for lr, batch in hp_tuning:\n",
    "    filename = f\"results_lr{lr}_batch{batch}.csv\"\n",
    "\n",
    "    c100_train_loader, c100_valid_loader, _ = data.getDataloader(\n",
    "    c100_trainset, c100_testset, valid_size, batch)\n",
    "    \n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Train KL Div'])\n",
    "        \n",
    "        bc100_net = BBBAlexNet(c100_outputs, c100_inputs, priors, activation_type='softplus').to(device)\n",
    "        bc100_criterion = metrics.ELBO(len(c100_trainset)).to(device)\n",
    "        bc100_optimizer = Adam(bc100_net.parameters(), lr=lr_start)\n",
    "        bc100_lr_sched = lr_scheduler.ReduceLROnPlateau(bc100_optimizer, patience=6, verbose=True)\n",
    "        bc100_valid_loss_max = np.Inf\n",
    "        \n",
    "        ckpt_name = f\"Bayesian/Models/bc100_lr{lr}_batch{batch}.pth\"\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            bc100_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            bc100_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            bc100_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            bc100_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0 \n",
    "        \n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):  # loop over the dataset multiple times\n",
    "        \n",
    "            bc100_train_loss, bc100_train_acc, bc100_train_kl = BCNN.train_model(bc100_net, bc100_optimizer, bc100_criterion, c100_train_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc100_valid_loss, bc100_valid_acc = BCNN.validate_model(bc100_net, bc100_criterion, c100_valid_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc100_lr_sched.step(bc100_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if bc100_valid_loss <= bc100_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    bc100_valid_loss_max, bc100_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': bc100_net.state_dict(),\n",
    "                    'optimizer_state_dict': bc100_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': bc100_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': bc100_valid_loss,\n",
    "                    'epoch': epoch  \n",
    "                }, ckpt_name)\n",
    "                bc100_valid_loss_max = bc100_valid_loss\n",
    "\n",
    "            writer.writerow([epoch, bc100_train_loss, bc100_train_acc, bc100_valid_loss, bc100_valid_acc, bc100_train_kl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79873-300a-410f-851d-1cb2f1e829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequentistCNN on CIFAR10\n",
    "fc10_net = AlexNet(c10_outputs, c10_inputs).to(device)\n",
    "fc10_criterion = nn.CrossEntropyLoss()\n",
    "fc10_optimizer = Adam(fc10_net.parameters(), lr=lr_start)\n",
    "fc10_lr_sched = lr_scheduler.ReduceLROnPlateau(fc10_optimizer, patience=6, verbose=True)\n",
    "fc10_valid_loss_max = np.Inf\n",
    "\n",
    "ckpt_name = 'Frequentist/Models/fc10.pth'\n",
    "if os.path.isfile(ckpt_name):\n",
    "    checkpoint = th.load(ckpt_name)\n",
    "    fc10_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    fc10_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    fc10_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    fc10_valid_loss_max = checkpoint['valid_loss_max']\n",
    "    print('Model loaded from {}'.format(ckpt_name))\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc10_train_loss, fc10_train_acc = FCNN.train_model(fc10_net, fc10_optimizer, fc10_criterion, c10_train_loader)\n",
    "    fc10_valid_loss, fc10_valid_acc = FCNN.validate_model(fc10_net, fc10_criterion, c10_valid_loader)\n",
    "    fc10_lr_sched.step(fc10_valid_loss)\n",
    "\n",
    "    # save model if validation accuracy has increased\n",
    "    if fc10_valid_loss <= fc10_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            fc10_valid_loss_max, fc10_valid_loss))\n",
    "        th.save({\n",
    "            'model_state_dict': fc10_net.state_dict(),\n",
    "            'optimizer_state_dict': fc10_optimizer.state_dict(),\n",
    "            'scheduler_state_dict': fc10_lr_sched.state_dict(),\n",
    "            'valid_loss_max': fc10_valid_loss\n",
    "        }, ckpt_name)\n",
    "        fc10_valid_loss_max = fc10_valid_loss\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc10_train_loss, fc10_train_acc, fc10_valid_loss, fc10_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f645b1-2c30-4fc8-aa1d-f32574fa7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequentist CNN on CIFAR100\n",
    "fc100_net = AlexNet(c100_outputs, c100_inputs).to(device)\n",
    "fc100_criterion = nn.CrossEntropyLoss()\n",
    "fc100_optimizer = Adam(fc100_net.parameters(), lr=lr_start)\n",
    "fc100_lr_sched = lr_scheduler.ReduceLROnPlateau(fc100_optimizer, patience=6, verbose=True)\n",
    "fc100_valid_loss_max = np.Inf\n",
    "\n",
    "ckpt_name = 'Frequentist/Models/fc100.pth'\n",
    "if os.path.isfile(ckpt_name):\n",
    "    checkpoint = th.load(ckpt_name)\n",
    "    fc100_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    fc100_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    fc100_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    fc100_valid_loss_max = checkpoint['valid_loss_max']\n",
    "    print('Model loaded from {}'.format(ckpt_name))\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc100_train_loss, fc100_train_acc = FCNN.train_model(fc100_net, fc100_optimizer, fc100_criterion, c100_train_loader)\n",
    "    fc100_valid_loss, fc100_valid_acc = FCNN.validate_model(fc100_net, fc100_criterion, c100_valid_loader)\n",
    "    fc100_lr_sched.step(fc100_valid_loss)\n",
    "\n",
    "    # save model if validation accuracy has increased\n",
    "    if fc100_valid_loss <= fc100_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            fc100_valid_loss_max, fc100_valid_loss))\n",
    "        th.save({\n",
    "            'model_state_dict': fc100_net.state_dict(),\n",
    "            'optimizer_state_dict': fc100_optimizer.state_dict(),\n",
    "            'scheduler_state_dict': fc100_lr_sched.state_dict(),\n",
    "            'valid_loss_max': fc100_valid_loss\n",
    "        }, ckpt_name)\n",
    "        fc100_valid_loss_max = fc100_valid_loss\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc100_train_loss, fc100_train_acc, fc100_valid_loss, fc100_valid_acc))\n",
    "\n",
    "# After all epochs are complete, evaluate the model on the test set\n",
    "fc100_test_loss, fc100_test_acc = FCNN.validate_model(fc100_net, fc100_criterion, c100_test_loader)\n",
    "print('Test Loss: {:.4f} \\tTest Accuracy: {:.4f}'.format(fc100_test_loss, fc100_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5926854-4df8-4789-af36-5b497ef4f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mislabel 10% of the CIFAR10 and CIFAR100 trainsets\n",
    "c10_trainset_mislabel = data.mislabel_data(c10_trainset, c10_outputs, 0.1)\n",
    "c100_trainset_mislabel = data.mislabel_data(c100_trainset, c100_outputs, 0.1)\n",
    "\n",
    "# Create new dataloaders for the mislabeled datasets\n",
    "c10_train_loader_mislabel, _, _ = data.getDataloader(\n",
    "    c10_trainset_mislabel, c10_testset, valid_size, batch_size)\n",
    "c100_train_loader_mislabel, _, _ = data.getDataloader(\n",
    "    c100_trainset_mislabel, c100_testset, valid_size, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
