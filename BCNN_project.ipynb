{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61780bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import data, metrics\n",
    "import Frequentist_main as FCNN\n",
    "import Bayesian_main as BCNN\n",
    "from Bayesian.BayesianCNN import BBBAlexNet\n",
    "from Frequentist.FrequentistCNN import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0e982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "\n",
    "n_epochs = 50\n",
    "lr_start = 0.001\n",
    "num_workers = 4\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc632e0-833b-470d-9139-824f2bfcb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader\n",
    "c10_trainset, c10_testset, c10_inputs, c10_outputs = data.getDataset('CIFAR10')\n",
    "c10_train_loader, c10_valid_loader, c10_test_loader = data.getDataloader(\n",
    "    c10_trainset, c10_testset, valid_size, batch_size, num_workers)\n",
    "\n",
    "c100_trainset, c100_testset, c100_inputs, c100_outputs = data.getDataset('CIFAR100')\n",
    "c100_train_loader, c100_valid_loader, c100_test_loader = data.getDataloader(\n",
    "    c100_trainset, c100_testset, valid_size, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6789e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BayesianCNN with softplus on CIFAR10\n",
    "bc10_sp_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='softplus').to(device)\n",
    "bc10_sp_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "bc10_sp_optimizer = Adam(bc10_sp_net.parameters(), lr=lr_start)\n",
    "bc10_sp_lr_sched = lr_scheduler.ReduceLROnPlateau(bc10_sp_optimizer, patience=6, verbose=True)\n",
    "bc10_sp_valid_loss_max = np.Inf\n",
    "ckpt_name = 'Bayesian/Models/bc10_sp.pth'\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc10_sp_train_loss, bc10_sp_train_acc, bc10_sp_train_kl = BCNN.train_model(bc10_sp_net, bc10_sp_optimizer, bc10_sp_criterion, c10_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc10_sp_valid_loss, bc10_sp_valid_acc = BCNN.validate_model(bc10_sp_net, bc10_sp_criterion, c10_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc10_sp_lr_sched.step(bc10_sp_valid_loss)\n",
    "\n",
    "     # save model if validation accuracy has increased\n",
    "    if bc10_sp_valid_loss <= bc10_sp_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            bc10_sp_valid_loss_max, bc10_sp_valid_loss))\n",
    "        th.save(bc10_sp_net.state_dict(), ckpt_name)\n",
    "        bc10_sp_valid_loss_max = bc10_sp_valid_loss\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc10_sp_train_loss, bc10_sp_train_acc, bc10_sp_valid_loss, bc10_sp_valid_acc, bc10_sp_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891eb21e-d97f-40e2-b041-524dc174b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BayesianCNN with softplus on CIFAR100\n",
    "bc100_sp_net = BBBAlexNet(c100_outputs, c100_inputs, priors, activation_type='softplus').to(device)\n",
    "bc100_sp_criterion = metrics.ELBO(len(c100_trainset)).to(device)\n",
    "bc100_sp_optimizer = Adam(bc100_sp_net.parameters(), lr=lr_start)\n",
    "bc100_sp_lr_sched = lr_scheduler.ReduceLROnPlateau(bc100_sp_optimizer, patience=6, verbose=True)\n",
    "bc100_sp_valid_loss_max = np.Inf\n",
    "ckpt_name = 'Bayesian/Models/bc100_sp.pth'\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc100_sp_train_loss, bc100_sp_train_acc, bc100_sp_train_kl = BCNN.train_model(bc100_sp_net, bc100_sp_optimizer, bc100_sp_criterion, c100_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc100_sp_valid_loss, bc100_sp_valid_acc = BCNN.validate_model(bc100_sp_net, bc100_sp_criterion, c100_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc100_sp_lr_sched.step(bc100_sp_valid_loss)\n",
    "\n",
    "    # save model if validation accuracy has increased\n",
    "    if bc100_sp_valid_loss <= bc100_sp_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            bc100_sp_valid_loss_max, bc100_sp_valid_loss))\n",
    "        th.save(bc100_sp_net.state_dict(), ckpt_name)\n",
    "        bc100_sp_valid_loss_max = bc100_sp_valid_loss\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc100_sp_train_loss, bc100_sp_train_acc, bc100_sp_valid_loss, bc100_sp_valid_acc, bc100_sp_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750fe94-db5a-4d5b-968e-fbc28d3bb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianCNN with relu on CIFAR10\n",
    "bc10_rl_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='relu').to(device)\n",
    "bc10_rl_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "bc10_rl_optimizer = Adam(bc10_rl_net.parameters(), lr=lr_start)\n",
    "bc10_rl_lr_sched = lr_scheduler.ReduceLROnPlateau(bc10_rl_optimizer, patience=6, verbose=True)\n",
    "bc10_rl_valid_loss_max = np.Inf\n",
    "ckpt_name = 'Bayesian/Models/bc10_rl.pth'\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc10_rl_train_loss, bc10_rl_train_acc, bc10_rl_train_kl = BCNN.train_model(bc10_rl_net, bc10_rl_optimizer, bc10_rl_criterion, c10_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc10_rl_valid_loss, bc10_rl_valid_acc = BCNN.validate_model(bc10_rl_net, bc10_rl_criterion, c10_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc10_rl_lr_sched.step(bc10_rl_valid_loss)\n",
    "\n",
    "    # save model if validation accuracy has increased\n",
    "    if bc10_rl_valid_loss <= bc10_rl_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            bc10_rl_valid_loss_max, bc10_rl_valid_loss))\n",
    "        th.save(bc10_rl_net.state_dict(), ckpt_name)\n",
    "        bc10_rl_valid_loss_max = bc10_rl_valid_loss\n",
    "    \n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc10_rl_train_loss, bc10_rl_train_acc, bc10_rl_valid_loss, bc10_rl_valid_acc, bc10_rl_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8afc2-14dd-4aeb-8242-36f950285847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianCNN with relu on CIFAR100\n",
    "bc100_rl_net = BBBAlexNet(c100_outputs, c100_inputs, priors, activation_type='relu').to(device)\n",
    "bc100_rl_criterion = metrics.ELBO(len(c100_trainset)).to(device)\n",
    "bc100_rl_optimizer = Adam(bc100_rl_net.parameters(), lr=lr_start)\n",
    "bc100_rl_lr_sched = lr_scheduler.ReduceLROnPlateau(bc100_rl_optimizer, patience=6, verbose=True)\n",
    "bc100_rl_valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc100_rl_train_loss, bc100_rl_train_acc, bc100_rl_train_kl = BCNN.train_model(bc100_rl_net, bc100_rl_optimizer, bc100_rl_criterion, c100_train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc100_rl_valid_loss, bc100_rl_valid_acc = BCNN.validate_model(bc100_rl_net, bc100_rl_criterion, c100_valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc100_rl_lr_sched.step(bc100_rl_valid_loss)\n",
    "\n",
    "    # save model if validation accuracy has increased\n",
    "    if bc100_rl_valid_loss <= bc100_rl_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            bc100_rl_valid_loss_max, bc100_rl_valid_loss))\n",
    "        th.save(bc100_rl_net.state_dict(), ckpt_name)\n",
    "        bc100_rl_valid_loss_max = bc100_rl_valid_loss\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc100_rl_train_loss, bc100_rl_train_acc, bc100_rl_valid_loss, bc100_rl_valid_acc, bc100_rl_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79873-300a-410f-851d-1cb2f1e829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequentistCNN on CIFAR10\n",
    "fc10_net = AlexNet(c10_outputs, c10_inputs).to(device)\n",
    "fc10_criterion = nn.CrossEntropyLoss()\n",
    "fc10_optimizer = Adam(fc10_net.parameters(), lr=lr_start)\n",
    "fc10_lr_sched = lr_scheduler.ReduceLROnPlateau(fc10_optimizer, patience=6, verbose=True)\n",
    "fc10_valid_loss_max = np.Inf\n",
    "ckpt_name = 'Frequentist/Models/fc10.pth'\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc10_train_loss, fc10_train_acc = FCNN.train_model(fc10_net, fc10_optimizer, fc10_criterion, c10_train_loader)\n",
    "    fc10_valid_loss, fc10_valid_acc = FCNN.validate_model(fc10_net, fc10_criterion, c10_valid_loader)\n",
    "    fc10_lr_sched.step(fc10_valid_loss)\n",
    "\n",
    "    # save model if validation accuracy has increased\n",
    "    if fc10_valid_loss <= fc10_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            fc10_valid_loss_max, fc10_valid_loss))\n",
    "        th.save(fc10_net.state_dict(), ckpt_name)\n",
    "        fc10_valid_loss_max = fc10_valid_loss\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc10_train_loss, fc10_train_acc, fc10_valid_loss, fc10_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f645b1-2c30-4fc8-aa1d-f32574fa7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequentist CNN on CIFAR100\n",
    "fc100_net = AlexNet(c100_outputs, c100_inputs).to(device)\n",
    "fc100_criterion = nn.CrossEntropyLoss()\n",
    "fc100_optimizer = Adam(fc100_net.parameters(), lr=lr_start)\n",
    "fc100_lr_sched = lr_scheduler.ReduceLROnPlateau(fc100_optimizer, patience=6, verbose=True)\n",
    "fc100_valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc100_train_loss, fc100_train_acc = FCNN.train_model(fc100_net, fc100_optimizer, fc100_criterion, c100_train_loader)\n",
    "    fc100_valid_loss, fc100_valid_acc = FCNN.validate_model(fc100_net, fc100_criterion, c100_valid_loader)\n",
    "    fc100_lr_sched.step(fc100_valid_loss)\n",
    "\n",
    "    # save model if validation accuracy has increased\n",
    "    if fc100_valid_loss <= fc100_valid_loss_max:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            fc100_valid_loss_max, fc100_valid_loss))\n",
    "        th.save(fc100_net.state_dict(), ckpt_name)\n",
    "        fc100_valid_loss_max = fc100_valid_loss\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc100_train_loss, fc100_train_acc, fc100_valid_loss, fc100_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33529bcf-0a43-47f1-8035-5c016b3fe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, n_epochs)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, bc10_sp_train_acc, label='bc10_sp')\n",
    "plt.plot(epochs, bc100_sp_train_acc, label='bc100_sp')\n",
    "plt.plot(epochs, bc10_rl_train_acc, label='bc10_rl')\n",
    "plt.plot(epochs, bc100_rl_train_acc, label='bc100_rl')\n",
    "plt.plot(epochs, fc10_train_acc, label='fc10')\n",
    "plt.plot(epochs, fc100_train_acc, label='fc100')\n",
    "\n",
    "plt.title('Comparison of training accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1caa85-a79d-4039-919b-5d2bde171785",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, n_epochs)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, bc10_sp_valid_acc, label='bc10_sp')\n",
    "plt.plot(epochs, bc100_sp_valid_acc, label='bc100_sp')\n",
    "plt.plot(epochs, bc10_rl_train_acc, label='bc10_rl')\n",
    "plt.plot(epochs, bc100_rl_train_acc, label='bc100_rl')\n",
    "plt.plot(epochs, fc10_valid_acc, label='fc10')\n",
    "plt.plot(epochs, fc100_valid_acc, label='fc100')\n",
    "\n",
    "plt.title('Comparison of validation accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
