{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61780bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from utils import data, metrics\n",
    "import Frequentist_main as FCNN\n",
    "import Bayesian_main as BCNN\n",
    "from Bayesian.BayesianCNN import BBBAlexNet\n",
    "from Frequentist.FrequentistCNN import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e0e982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca9cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_type = 'softplus'  # 'softplus' or 'relu'\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "\n",
    "n_epochs = 5\n",
    "lr_start = 0.001\n",
    "num_workers = 4\n",
    "valid_size = 0.2\n",
    "batch_size = 256\n",
    "train_ens = 1\n",
    "valid_ens = 1\n",
    "beta_type = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6789e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leona\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      " 20%|████████████████▌                                                                  | 1/5 [03:26<13:44, 206.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 38292053.3758 \tTraining Accuracy: 0.1493 \tValidation Loss: 30205720.9000 \tValidation Accuracy: 0.1738 \ttrain_kl_div: 376424420.0764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 2/5 [07:31<11:27, 229.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 25851155.1083 \tTraining Accuracy: 0.2234 \tValidation Loss: 22229799.9000 \tValidation Accuracy: 0.2506 \ttrain_kl_div: 257407123.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [11:42<07:58, 239.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 19645173.1083 \tTraining Accuracy: 0.2647 \tValidation Loss: 17365266.9500 \tValidation Accuracy: 0.2836 \ttrain_kl_div: 195440729.5796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [15:02<03:43, 223.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 15605968.3121 \tTraining Accuracy: 0.2819 \tValidation Loss: 14020036.7250 \tValidation Accuracy: 0.2635 \ttrain_kl_div: 155079582.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [18:17<00:00, 219.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 12740687.9108 \tTraining Accuracy: 0.2958 \tValidation Loss: 11573468.8500 \tValidation Accuracy: 0.2646 \ttrain_kl_div: 126435828.7389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainset, testset, inputs, outputs = data.getDataset('CIFAR10')\n",
    "train_loader, valid_loader, test_loader = data.getDataloader(\n",
    "    trainset, testset, valid_size, batch_size, num_workers)\n",
    "net = BBBAlexNet(outputs, inputs, priors, activation_type).to(device)\n",
    "criterion = metrics.ELBO(len(trainset)).to(device)\n",
    "optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc10_train_loss, bc10_train_acc, bc10_train_kl = BCNN.train_model(net, optimizer, criterion, train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc10_valid_loss, bc10_valid_acc = BCNN.validate_model(net, criterion, valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    lr_sched.step(valid_loss)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc10_train_loss, bc10_train_acc, bc10_valid_loss, bc10_valid_acc, bc10_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891eb21e-d97f-40e2-b041-524dc174b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, inputs, outputs = data.getDataset('CIFAR100')\n",
    "train_loader, valid_loader, test_loader = data.getDataloader(\n",
    "    trainset, testset, valid_size, batch_size, num_workers)\n",
    "net = BBBAlexNet(outputs, inputs, priors, activation_type).to(device)\n",
    "criterion = metrics.ELBO(len(trainset)).to(device)\n",
    "optimizer = Adam(net.parameters(), lr=lr_start)\n",
    "lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "valid_loss_max = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "    bc100_train_loss, bc100_train_acc, bc100_train_kl = BCNN.train_model(net, optimizer, criterion, train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    bc100_valid_loss, bc100_valid_acc = BCNN.validate_model(net, criterion, valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    lr_sched.step(valid_loss)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}'.format(\n",
    "        epoch, bc100_train_loss, bc100_train_acc, bc100_valid_loss, bc100_valid_acc, bc100_train_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79873-300a-410f-851d-1cb2f1e829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, inputs, outputs = data.getDataset('CIFAR10')\n",
    "train_loader, valid_loader, test_loader = data.getDataloader(\n",
    "    trainset, testset, valid_size, batch_size, num_workers)\n",
    "net = AlexNet.(outputs, inputs).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "valid_loss_min = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc10_train_loss, fc10_train_acc = FCNN.train_model(net, optimizer, criterion, train_loader)\n",
    "    fc10_valid_loss, fc10_valid_acc = FCNN.validate_model(net, criterion, valid_loader)\n",
    "    lr_sched.step(valid_loss)\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc10_train_loss, fc10_train_acc, fc10_valid_loss, fc10_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f645b1-2c30-4fc8-aa1d-f32574fa7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, inputs, outputs = data.getDataset('CIFAR100')\n",
    "train_loader, valid_loader, test_loader = data.getDataloader(\n",
    "    trainset, testset, valid_size, batch_size, num_workers)\n",
    "net = AlexNet.(outputs, inputs).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
    "valid_loss_min = np.Inf\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "    fc100_train_loss, fc100_train_acc = FCNN.train_model(net, optimizer, criterion, train_loader)\n",
    "    fc100_valid_loss, fc100_valid_acc = FCNN.validate_model(net, criterion, valid_loader)\n",
    "    lr_sched.step(valid_loss)\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\n",
    "        epoch, fc100_train_loss, fc100_train_acc, fc100_valid_loss, fc100_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33529bcf-0a43-47f1-8035-5c016b3fe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 201)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, bc10_train_acc, label='bc10')\n",
    "plt.plot(epochs, bc100_train_acc, label='bc100')\n",
    "plt.plot(epochs, fc10_train_acc, label='fc10')\n",
    "plt.plot(epochs, fc100_train_acc, label='fc100')\n",
    "\n",
    "plt.title('Comparison of training accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1caa85-a79d-4039-919b-5d2bde171785",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 201)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, bc10_valid_acc, label='bc10')\n",
    "plt.plot(epochs, bc100_valid_acc, label='bc100')\n",
    "plt.plot(epochs, fc10_valid_acc, label='fc10')\n",
    "plt.plot(epochs, fc100_valid_acc, label='fc100')\n",
    "\n",
    "plt.title('Comparison of validation accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
