{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61780bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils import data, metrics\n",
    "import Frequentist_main as FCNN\n",
    "import Bayesian_main as BCNN\n",
    "from Bayesian.BayesianCNN import BBBAlexNet\n",
    "from Frequentist.FrequentistCNN import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0e982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "priors={\n",
    "    'prior_mu': 0,\n",
    "    'prior_sigma': 0.1,\n",
    "    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n",
    "    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n",
    "}\n",
    "\n",
    "n_epochs = 100\n",
    "lr_start = [0.005, 0.001, 0.0005]\n",
    "batch_size_c = [128, 256, 512]\n",
    "hp_tuning = list(itertools.product(lr_start, batch_size_c))\n",
    "valid_size = 0.2\n",
    "beta_type = \"Blundell\" \n",
    "\n",
    "mislabel_perc = [0.05, 0.1, 0.2, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc632e0-833b-470d-9139-824f2bfcb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader\n",
    "c10_trainset, c10_testset, c10_inputs, c10_outputs = data.getDataset('CIFAR10')\n",
    "c100_trainset, c100_testset, c100_inputs, c100_outputs = data.getDataset('CIFAR100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6789e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BayesianCNN with softplus on CIFAR10 hyperparametrization tuning\n",
    "for lr, batch in hp_tuning:\n",
    "    filename = f\"results_c10_lr{lr}_batch{batch}.csv\"\n",
    "\n",
    "    c10_train_loader, c10_valid_loader, _ = data.getDataloader(\n",
    "    c10_trainset, c10_testset, valid_size, batch)\n",
    "    \n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Train KL Div'])\n",
    "        \n",
    "        bc10_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='softplus').to(device)\n",
    "        bc10_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "        bc10_optimizer = Adam(bc10_net.parameters(), lr=lr)\n",
    "        bc10_lr_sched = lr_scheduler.ReduceLROnPlateau(bc10_optimizer, patience=6, verbose=True)\n",
    "        bc10_valid_loss_max = np.Inf\n",
    "\n",
    "        ckpt_name = f\"Bayesian/Models/bc10_lr{lr}_batch{batch}.pth\"\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            bc10_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            bc10_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            bc10_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            bc10_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0 \n",
    "\n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):  # loop over the dataset multiple times\n",
    "        \n",
    "            bc10_train_loss, bc10_train_acc, bc10_train_kl = BCNN.train_model(bc10_net, bc10_optimizer, bc10_criterion, c10_train_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc10_valid_loss, bc10_valid_acc = BCNN.validate_model(bc10_net, bc10_criterion, c10_valid_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc10_lr_sched.step(bc10_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if bc10_valid_loss <= bc10_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    bc10_valid_loss_max, bc10_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': bc10_net.state_dict(),\n",
    "                    'optimizer_state_dict': bc10_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': bc10_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': bc10_valid_loss,\n",
    "                    'epoch': epoch\n",
    "                }, ckpt_name)\n",
    "                bc10_valid_loss_max = bc10_valid_loss\n",
    "\n",
    "            writer.writerow([epoch, bc10_train_loss, bc10_train_acc, bc10_valid_loss, bc10_valid_acc, bc10_train_kl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891eb21e-d97f-40e2-b041-524dc174b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BayesianCNN with softplus on CIFAR100\n",
    "for lr, batch in hp_tuning:\n",
    "    filename = f\"results_c100_lr{lr}_batch{batch}.csv\"\n",
    "\n",
    "    c100_train_loader, c100_valid_loader, _ = data.getDataloader(\n",
    "    c100_trainset, c100_testset, valid_size, batch)\n",
    "    \n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Train KL Div'])\n",
    "        \n",
    "        bc100_net = BBBAlexNet(c100_outputs, c100_inputs, priors, activation_type='softplus').to(device)\n",
    "        bc100_criterion = metrics.ELBO(len(c100_trainset)).to(device)\n",
    "        bc100_optimizer = Adam(bc100_net.parameters(), lr=lr_start)\n",
    "        bc100_lr_sched = lr_scheduler.ReduceLROnPlateau(bc100_optimizer, patience=6, verbose=True)\n",
    "        bc100_valid_loss_max = np.Inf\n",
    "        \n",
    "        ckpt_name = f\"Bayesian/Models/bc100_lr{lr}_batch{batch}.pth\"\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            bc100_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            bc100_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            bc100_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            bc100_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0 \n",
    "        \n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):  # loop over the dataset multiple times\n",
    "        \n",
    "            bc100_train_loss, bc100_train_acc, bc100_train_kl = BCNN.train_model(bc100_net, bc100_optimizer, bc100_criterion, c100_train_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc100_valid_loss, bc100_valid_acc = BCNN.validate_model(bc100_net, bc100_criterion, c100_valid_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc100_lr_sched.step(bc100_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if bc100_valid_loss <= bc100_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    bc100_valid_loss_max, bc100_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': bc100_net.state_dict(),\n",
    "                    'optimizer_state_dict': bc100_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': bc100_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': bc100_valid_loss,\n",
    "                    'epoch': epoch  \n",
    "                }, ckpt_name)\n",
    "                bc100_valid_loss_max = bc100_valid_loss\n",
    "\n",
    "            writer.writerow([epoch, bc100_train_loss, bc100_train_acc, bc100_valid_loss, bc100_valid_acc, bc100_train_kl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91868b5-bc71-415b-9852-37efb13f475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameter\n",
    "lr_start_c10 = 0.001\n",
    "lr_start_c100 = 0.0005\n",
    "batch_size = 128\n",
    "\n",
    "# DataLoader with best batch size\n",
    "c10_train_loader, c10_valid_loader, c10_test_loader = data.getDataloader(\n",
    "    c10_trainset, c10_testset, valid_size, batch_size)\n",
    "c100_train_loader, c100_valid_loader, c100_test_loader = data.getDataloader(\n",
    "    c100_trainset, c100_testset, valid_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79873-300a-410f-851d-1cb2f1e829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequentistCNN on CIFAR10\n",
    "filename = f\"results_f10_lr{lr}_batch{batch}.csv\"\n",
    "\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Train KL Div'])\n",
    "\n",
    "        fc10_net = AlexNet(c10_outputs, c10_inputs).to(device)\n",
    "        fc10_criterion = nn.CrossEntropyLoss()\n",
    "        fc10_optimizer = Adam(fc10_net.parameters(), lr=lr_start_c10)\n",
    "        fc10_lr_sched = lr_scheduler.ReduceLROnPlateau(fc10_optimizer, patience=6, verbose=True)\n",
    "        fc10_valid_loss_max = np.Inf\n",
    "        \n",
    "        ckpt_name = 'Frequentist/Models/fc10.pth'\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            fc10_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            fc10_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            fc10_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            fc10_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        \n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):\n",
    "        \n",
    "            fc10_train_loss, fc10_train_acc = FCNN.train_model(fc10_net, fc10_optimizer, fc10_criterion, c10_train_loader)\n",
    "            fc10_valid_loss, fc10_valid_acc = FCNN.validate_model(fc10_net, fc10_criterion, c10_valid_loader)\n",
    "            fc10_lr_sched.step(fc10_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if fc10_valid_loss <= fc10_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    fc10_valid_loss_max, fc10_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': fc10_net.state_dict(),\n",
    "                    'optimizer_state_dict': fc10_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': fc10_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': fc10_valid_loss,\n",
    "                    'epoch': epoch\n",
    "                }, ckpt_name)\n",
    "                fc10_valid_loss_max = fc10_valid_loss\n",
    "\n",
    "            writer.writerow([epoch, fc10_train_loss, fc10_train_acc, fc10_valid_loss, fc10_valid_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f645b1-2c30-4fc8-aa1d-f32574fa7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequentist CNN on CIFAR100\n",
    "filename = f\"results_f100_lr{lr}_batch{batch}.csv\"\n",
    "\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Train KL Div'])\n",
    "\n",
    "        fc100_net = AlexNet(c100_outputs, c100_inputs).to(device)\n",
    "        fc100_criterion = nn.CrossEntropyLoss()\n",
    "        fc100_optimizer = Adam(fc100_net.parameters(), lr=lr_start_c100)\n",
    "        fc100_lr_sched = lr_scheduler.ReduceLROnPlateau(fc100_optimizer, patience=6, verbose=True)\n",
    "        fc100_valid_loss_max = np.Inf\n",
    "        \n",
    "        ckpt_name = 'Frequentist/Models/fc100.pth'\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            fc100_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            fc100_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            fc100_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            fc100_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        \n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):\n",
    "        \n",
    "            fc100_train_loss, fc100_train_acc = FCNN.train_model(fc100_net, fc100_optimizer, fc100_criterion, c100_train_loader)\n",
    "            fc100_valid_loss, fc100_valid_acc = FCNN.validate_model(fc100_net, fc100_criterion, c100_valid_loader)\n",
    "            fc100_lr_sched.step(fc100_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if fc100_valid_loss <= fc100_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    fc100_valid_loss_max, fc100_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': fc100_net.state_dict(),\n",
    "                    'optimizer_state_dict': fc100_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': fc100_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': fc100_valid_loss,\n",
    "                    'epoch': epoch\n",
    "                }, ckpt_name)\n",
    "                fc100_valid_loss_max = fc100_valid_loss\n",
    "                    \n",
    "            writer.writerow([epoch, fc100_train_loss, fc100_train_acc, fc100_valid_loss, fc100_valid_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ec7b5-9523-47f3-889d-2d098de1e551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "bc10_checkpoint = th.load('Bayesian/Models/bc10_lr0.001_batch128.pth', map_location=th.device('cpu'))\n",
    "bc10_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='softplus').to(device)\n",
    "bc10_net.load_state_dict(bc10_checkpoint['model_state_dict'])\n",
    "bc100_checkpoint = th.load('Bayesian/Models/bc100_lr0.0005_batch128.pth', map_location=th.device('cpu'))\n",
    "bc100_net = BBBAlexNet(c100_outputs, c100_inputs, priors, activation_type='softplus').to(device)\n",
    "bc100_net.load_state_dict(bc100_checkpoint['model_state_dict'])\n",
    "fc10_checkpoint = th.load('Frequentist/Models/fc10.pth', map_location=th.device('cpu'))\n",
    "fc10_net = AlexNet(c10_outputs, c10_inputs).to(device)\n",
    "fc10_net.load_state_dict(fc10_checkpoint['model_state_dict'])\n",
    "fc100_checkpoint = th.load('Frequentist/Models/fc100.pth', map_location=th.device('cpu'))\n",
    "fc100_net = AlexNet(c100_outputs, c100_inputs).to(device)\n",
    "fc100_net.load_state_dict(fc100_checkpoint['model_state_dict'])\n",
    "\n",
    "# Define the criterion\n",
    "bc10_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "bc100_criterion = metrics.ELBO(len(c100_trainset)).to(device)\n",
    "f_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the test loss and accuracy\n",
    "bc10_test_loss, bc10_test_acc = BCNN.validate_model(bc10_net, bc10_criterion, c10_test_loader)\n",
    "bc100_test_loss, bc100_test_acc = BCNN.validate_model(bc100_net, bc100_criterion, c100_test_loader)\n",
    "fc10_test_loss, fc10_test_acc = FCNN.validate_model(fc10_net, f_criterion, c10_test_loader)\n",
    "fc100_test_loss, fc100_test_acc = FCNN.validate_model(fc100_net, f_criterion, c100_test_loader)\n",
    "\n",
    "test_loss = [bc10_test_loss, bc100_test_loss, fc10_test_loss, fc100_test_loss]\n",
    "test_acc = [bc10_test_acc, bc100_test_acc, fc10_test_acc, fc100_test_acc]\n",
    "names = ['BC10', 'BC100', 'FC10', 'FC100']\n",
    "\n",
    "# Print the test accuracies\n",
    "for name, loss, acc in zip(names, test_loss, test_acc):\n",
    "    print(f'Model: {name}, Test Loss: {loss:.6f}, Test Accuracy: {acc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c919d9-153b-4314-ba4c-414782fcc817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianCNN with softplus on CIFAR10 hyperparametrization tuning\n",
    "for perc in mislabel_perc:\n",
    "    filename = f\"results_bayesian_mislabel_{perc}.csv\"\n",
    "\n",
    "    # Mislabel of the CIFAR10 trainset\n",
    "    c10_train_loader_mislabel, c10_valid_loader, c10_test_loader, _ = data.getDataloader_mislabel(\n",
    "        c10_trainset, c10_testset, valid_size, batch_size, mislabel_percentage=perc)\n",
    "    \n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy', 'Train KL Div'])\n",
    "        \n",
    "        bc10_net = BBBAlexNet(c10_outputs, c10_inputs, priors, activation_type='softplus').to(device)\n",
    "        bc10_criterion = metrics.ELBO(len(c10_trainset)).to(device)\n",
    "        bc10_optimizer = Adam(bc10_net.parameters(), lr=lr_start_c10)\n",
    "        bc10_lr_sched = lr_scheduler.ReduceLROnPlateau(bc10_optimizer, patience=6, verbose=True)\n",
    "        bc10_valid_loss_max = np.Inf\n",
    "\n",
    "        ckpt_name = f\"Bayesian/Models/bayesian_mislabel_{perc}.pth\"\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            bc10_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            bc10_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            bc10_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            bc10_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0 \n",
    "\n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):  # loop over the dataset multiple times\n",
    "        \n",
    "            bc10_train_loss, bc10_train_acc, bc10_train_kl = BCNN.train_model(bc10_net, bc10_optimizer, bc10_criterion, c10_train_loader_mislabel, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc10_valid_loss, bc10_valid_acc = BCNN.validate_model(bc10_net, bc10_criterion, c10_valid_loader, num_ens=1, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "            bc10_lr_sched.step(bc10_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if bc10_valid_loss <= bc10_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    bc10_valid_loss_max, bc10_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': bc10_net.state_dict(),\n",
    "                    'optimizer_state_dict': bc10_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': bc10_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': bc10_valid_loss,\n",
    "                    'epoch': epoch\n",
    "                }, ckpt_name)\n",
    "                bc10_valid_loss_max = bc10_valid_loss\n",
    "\n",
    "            writer.writerow([epoch, bc10_train_loss, bc10_train_acc, bc10_valid_loss, bc10_valid_acc, bc10_train_kl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50b538-305d-4fa4-afbd-e59e88f75033",
   "metadata": {},
   "outputs": [],
   "source": [
    "for perc in mislabel_perc:\n",
    "    filename = f\"results_frequentist_mislabel_{perc}.csv\"\n",
    "\n",
    "    # Mislabel of the CIFAR10 trainset\n",
    "    c10_train_loader_mislabel, c10_valid_loader, c10_test_loader, _ = data.getDataloader_mislabel(\n",
    "        c10_trainset, c10_testset, valid_size, batch_size, mislabel_percentage=perc)\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy'])\n",
    "\n",
    "        fc10_net = AlexNet(c10_outputs, c10_inputs).to(device)\n",
    "        fc10_criterion = nn.CrossEntropyLoss()\n",
    "        fc10_optimizer = Adam(fc10_net.parameters(), lr=lr_start_c10)\n",
    "        fc10_lr_sched = lr_scheduler.ReduceLROnPlateau(fc10_optimizer, patience=6, verbose=True)\n",
    "        fc10_valid_loss_max = np.Inf\n",
    "        \n",
    "        ckpt_name = f'Frequentist/Models/frequentist_mislabel_{perc}.pth'\n",
    "        if os.path.isfile(ckpt_name):\n",
    "            checkpoint = th.load(ckpt_name)\n",
    "            fc10_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            fc10_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            fc10_lr_sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            fc10_valid_loss_max = checkpoint['valid_loss_max']\n",
    "            start_epoch = checkpoint['epoch'] + 1  \n",
    "            print('Model loaded from {}'.format(ckpt_name))\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        \n",
    "        for epoch in tqdm(range(start_epoch, n_epochs)):\n",
    "        \n",
    "            fc10_train_loss, fc10_train_acc = FCNN.train_model(fc10_net, fc10_optimizer, fc10_criterion, c10_train_loader_mislabel)\n",
    "            fc10_valid_loss, fc10_valid_acc = FCNN.validate_model(fc10_net, fc10_criterion, c10_valid_loader)\n",
    "            fc10_lr_sched.step(fc10_valid_loss)\n",
    "        \n",
    "            # save model if validation accuracy has increased\n",
    "            if fc10_valid_loss <= fc10_valid_loss_max:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    fc10_valid_loss_max, fc10_valid_loss))\n",
    "                th.save({\n",
    "                    'model_state_dict': fc10_net.state_dict(),\n",
    "                    'optimizer_state_dict': fc10_optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': fc10_lr_sched.state_dict(),\n",
    "                    'valid_loss_max': fc10_valid_loss,\n",
    "                    'epoch': epoch\n",
    "                }, ckpt_name)\n",
    "                fc10_valid_loss_max = fc10_valid_loss\n",
    "\n",
    "            writer.writerow([epoch, fc10_train_loss, fc10_train_acc, fc10_valid_loss, fc10_valid_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecaa70-7472-4191-b36b-394c3ef04c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check that is right mislabelled\n",
    "train_loader_mis, valid_loader, test_loader, train_loader = data.getDataloader_mislabel(c10_trainset, c10_testset, valid_size, batch_size, mislabel_percentage=0.2)\n",
    "original_labels = np.array(train_loader.dataset.targets)\n",
    "mislabeled_labels = np.array(train_loader_mis.dataset.targets)\n",
    "\n",
    "# Check the percentage of mislabeled labels\n",
    "mislabeled_count = np.sum(original_labels != mislabeled_labels)\n",
    "actual_mislabel_percentage = mislabeled_count / len(original_labels)\n",
    "\n",
    "print(f\"Expected mislabel percentage: {0.2}\")\n",
    "print(f\"Actual mislabel percentage: {actual_mislabel_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7ffef-8e13-4e75-a2c6-6241982f79cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
